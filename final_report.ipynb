{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this report is to present the work carried out as part of our “Mobile Robotics” project at EPFL. Our team is composed of 4 students from the Robotics Master: Christian Matala, Samy Gaillard, Alix Papadatos and Zacharie Bourlard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Project's goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of our project is to learn how to implement robot navigation with the help of a camera. The challenge is to relate the robot's data to that provided by the camera in order to get the most accurate motion control. For that, we need to link several modules together: computer vision, filtering, global navigation and local navigation. Initially, we dealt with each of these modules separately before grouping them together in a single code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Project's constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imposed Material:**\n",
    "- Use of the Thymio Robot \n",
    "- Use of the supplied camera\n",
    "\n",
    "**Global motion constraints:**\n",
    "- Vision: \n",
    "    - Environment generation\n",
    "    - Best path generation\n",
    "    - Fixed obstacles avoidance \n",
    "- Filtering (position estimation)\n",
    "- Local obstacles avoidance\n",
    "\n",
    "**Additionnal challenges:**\n",
    "- Good response to \"Kidnapping\" of the robot\n",
    "- Good response to end-point displacement\n",
    "- Good response to camera obstruction (showing good filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought our setup with the idea in mind to create a stable setup that can be installed in many different conditions. We didn't want to be dependent on the level or shade of lighting in the room we were in. So we didn't want to distinguish our robot, obstacles and the boundaries of our environment by color, but rather by geometric distinctions: the Aruco codes. This is how we created the following **map** for our project:  <<image_map>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the camera, we used a microphone **tripod** that allowed us to be very flexible concerning the possible locations where we can install our setup. <<image_tripod>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will analyze the structure of the Computer vision implementation for our project. For this we will start by enumerating the different external libraries that we used, then we will explain how we initialize our scene in our code, to finally detail the methods used to detect the obstacle, the robot and the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 External Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer vision we used external libraries to help us in various tasks, we thus reference them here before introducing our implementation.\n",
    "\n",
    "We used:\n",
    "1. OpenCV-Contrib-Python which is a library that contains all opencv functions plus some funcitons added by contributors. OpenCV provided a rich set of functions that helped us process the images of our camera.\n",
    "2. Numpy library for its easy data structures and mathematical operations.\n",
    "3. Aruco library to handle the detection of our Aruco tags.\n",
    "4. Scipy to handle transformation between rotation matrixs,quaternions and Euler angles.\n",
    "\n",
    "NOTE: from now on the term \"tag\" or \"marker\" refers to ArUCo Markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vision module structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computer vision module is divided into several sub modules, shown on the diagram below:\n",
    "vision.py is the main file containing the Map class definition, its attributes and functions to be called in the main and through other modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scene initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scene, we chose to create a Map class, which will be in charge of handing every vision variable and functions as attributes and methods, as shown below (which only contains the class attributes, the methods will be shown further below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from constants import *\n",
    "\n",
    "class Map:\n",
    "    def __init__(self):\n",
    "        # Initialilze camera video capture\n",
    "        self.capture = cv.VideoCapture(0)\n",
    "        # Drop the first x frames\n",
    "        for _ in range(FIRST_FRAME):\n",
    "            self.capture.read()\n",
    "        \n",
    "        # Define class attributes\n",
    "        self.success = True\n",
    "        self.raw_frame = None\n",
    "        self.frame = None\n",
    "        self.robot = 3*[None]\n",
    "        self.destination = 3*[None]\n",
    "        self.map_corners = {}\n",
    "        self.found_corners = False\n",
    "        self.found_robot = False\n",
    "        self.found_destination = False\n",
    "        self.obstacles = []\n",
    "        self.obstacles_lines = []\n",
    "        self.target_lines = []\n",
    "        self.pose_est = 3*[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Map class starts by initializing a capture for our camera to take pictures from, using opencv's method VideoCapture.\n",
    "- Then it drops the first X frame to let the camera adapt its settings to the environment.\n",
    "- It then initializes the rest of the class attributes:\n",
    "1. success: False if it fails an image treatment\n",
    "2. raw_frame: Raw captured frame of the camera\n",
    "3. frame: Raw frame treated\n",
    "4. robot: Contains the robot position in (x,y) and its orientation.\n",
    "5. destination: Contains the destination's position in (x,y) and the corners of the detected aruco tag\n",
    "6. map_corners: if the aruco tags in the corners are detected, this maps them to the pixel that we will use to crop the image\n",
    "7. found_corners: is true if the aruco tags in the corners are detected\n",
    "8. found_robot: is true if the robot tag is detected\n",
    "9. found_destination: is true if the destination tag is detected\n",
    "10. obstacles: list of obstacles, each defined as a list of points\n",
    "11. possible_lines: list of lines of the computed visibility graph\n",
    "12. target_lines: list of points that creates the computed shortest path\n",
    "13. pose_est: position estimation of the kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Detection of obstacles, Thymio and goal point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll describe three important steps. The first one is the visibility matrix, it helps the robot see its surroundings by figuring out what’s in its line of sight. The second one is the path planning, where the robot figures out the best route to reach its goal while avoiding obstacles. Then we have the robot's actuation which is how the robot actually moves along the planned path.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Visibility matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We chose the visibility graph approach because it’s simple and finds the shortest path while avoiding obstacles. It’s a practical and efficient method that works well for this kind of navigation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the visibility matrix we used the Shapely library, we imported LineString and Polygon. LineString is used to represent the line segments between pairs of corners while Polygon is used to represents the abstacles as polygons. Those allow us to perform geometric operations such as testing whether a line intersects the obstacle, touches its boundary, or is fully contained within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString, Polygon\n",
    "\n",
    "def compute_visibility_matrix(start,end,obstacles):\n",
    "    \"\"\"\n",
    "    Compute a visibility matrix\n",
    "\n",
    "    input:\n",
    "        start point : start position of the robot\n",
    "        end point : end point; where the robot needs to stop\n",
    "        obstacles: List of obstacles, each as a list of extended \n",
    "                   corner coordinates [[(x1, y1), ...], ...]\n",
    "    \n",
    "    output:\n",
    "        Visibility_mat : (N x N) Visibility_mat(i,j) is equal to 1 if a path exist between the ith corner\n",
    "                            and the jth corner, if not  it's equal to 0.\n",
    "        corners: (1 X N) a list of corners\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert start and end points to single-point obstacles\n",
    "    start_obstacle = [start]\n",
    "    end_obstacle = [end]\n",
    "\n",
    "    # Add start and end points to the obstacles list\n",
    "    obstacles = [start_obstacle, end_obstacle] + obstacles\n",
    "\n",
    "    # Flatten the list of obstacles to get all corners\n",
    "    corners = [corner for obstacle in obstacles for corner in obstacle]\n",
    "    corners = np.array(corners)\n",
    "    N = len(corners)\n",
    "    matrix = np.ones((N, N), dtype=int)\n",
    "\n",
    "    # Convert obstacles to polygons\n",
    "\n",
    "    obstacle_polygons = []\n",
    "    for obs in obstacles:\n",
    "        if len(obs) >= 3:\n",
    "            obstacle_polygons.append(Polygon(obs))\n",
    "    obstacle_polygons = np.array(obstacle_polygons)\n",
    "\n",
    "    # Map corners to their respective obstacle indices\n",
    "    corner_to_obstacle = {}\n",
    "    for obs_idx, obstacle in enumerate(obstacles):\n",
    "        for corner in obstacle:\n",
    "            corner_to_obstacle[tuple(corner)] = (obs_idx, obstacle)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            line = LineString([corners[i], corners[j]])\n",
    "            # Check if corners[i] and corners[j] belong to the same obstacle\n",
    "            same_obstacle = (\n",
    "                tuple(corners[i]) in corner_to_obstacle \n",
    "                and tuple(corners[j]) in corner_to_obstacle \n",
    "                and corner_to_obstacle[tuple(corners[i])][0] == corner_to_obstacle[tuple(corners[j])][0]\n",
    "            )\n",
    "\n",
    "            # If they are from the same obstacle\n",
    "            if same_obstacle:\n",
    "                # Retrieve the obstacle corners\n",
    "                obstacle_corners = corner_to_obstacle[tuple(corners[i])][1]\n",
    "                # Check if they are adjacent corners (only adjacent corners are visible)\n",
    "                if not are_adjacent_corners(corners[i], corners[j], obstacle_corners):\n",
    "                    matrix[i, j] = 0\n",
    "\n",
    "            else:\n",
    "                for poly in obstacle_polygons:\n",
    "                    if line.intersects(poly) and not line.touches(poly.boundary):\n",
    "                        matrix[i, j] = 0\n",
    "                        break\n",
    "\n",
    "\n",
    "    return matrix, corners\n",
    "    \n",
    "\n",
    "def are_adjacent_corners(corner1, corner2, obstacle_corners):\n",
    "    \"\"\"\n",
    "    Check if two corners are adjacent in the obstacle.\n",
    "\n",
    "    input:\n",
    "        corner1: First corner (x, y)\n",
    "        corner2: Second corner (x, y)\n",
    "        obstacle_corners: List of corners of the obstacle\n",
    "    \n",
    "    output:\n",
    "        True if the corners are adjacent, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(obstacle_corners)\n",
    "    for i in range(n):\n",
    "        if (np.array_equal(obstacle_corners[i], corner1) and \n",
    "           (np.array_equal(obstacle_corners[(i + 1) % n], corner2) or \n",
    "           np.array_equal(obstacle_corners[(i - 1) % n], corner2))):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHOTO LIGNE POSSIBLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Path planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to implement the A* algorithm for the path planning because it's very efficient, it also reduces the unnecessary exploration by using the heuristic function.\n",
    "\n",
    "The algorithm starts by initializing a priority queue with the start node, where each node is scored based on the total cost (f_cost = g_cost + heuristic). At each step, the node with the lowest f_cost is processed. If the goal is reached, the path is reconstructed by tracing back through the came_from dictionary.\n",
    "\n",
    "For each neighbor of the current node, the algorithm calculates the cost (g_cost). If this cost is lower than a previously recorded cost, the neighbor is updated in the priority queue. The process continues until the goal is reached or no valid path is found, ensuring an optimal path with minimal computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(p1, p2):\n",
    "    # Implement the Manhattan distance heuristic\n",
    "    return abs(p1[0] - p2[0]) + abs(p1[1] - p2[1])\n",
    "\n",
    "\n",
    "\n",
    "def a_star_search(points,ex_path):\n",
    "    \"\"\"\n",
    "    A* \n",
    "\n",
    "    input:\n",
    "        start\n",
    "        end \n",
    "        points: N x 2 array (N = (# of corners) ), where:\n",
    "            - The remaining rows contain the coordinates of the extended corners.\n",
    "        ex_path: N X N, is equal to 1 if two corners are directly connected \n",
    "              by a line that does not cross any obstacle, and 0 otherwise.\n",
    "\n",
    "    output: \n",
    "        shortest_path: M X 2 (M = # of corners in the shortest path), A list \n",
    "            of corner indices representing the shortest path from start to goal.\n",
    "\n",
    "    Note: This implementation is heavily inspired by the A* algorithm provided in \n",
    "      the solution to the 5th exercise session of the Mobile Robotics course.\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize the open set as a priority queue and add the start node\n",
    "    open_set = []\n",
    "    start_index = 0\n",
    "    start = points[0,:]\n",
    "    goal_index = 1\n",
    "    goal = points[1,:]\n",
    "\n",
    "    N = points.shape[0]\n",
    "    distance_matrix = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "         for j in range(N):\n",
    "              #calculate the distance between two corners\n",
    "              distance_matrix[i,j] = norm(points[i,:]-points[j,])\n",
    "    \n",
    "\n",
    "    heappush(open_set, (heuristic(start, goal), 0, start_index))  # (f_cost, g_cost, position)\n",
    "\n",
    "    # Initialize the came_from dictionary\n",
    "    came_from = {}\n",
    "    # Initialize g_costs dictionary with default value of infinity and set g_costs[start] = 0\n",
    "    g_costs = {start_index: 0}\n",
    "    # Initialize the explored set\n",
    "    explored = set()\n",
    "    operation_count = 0\n",
    "\n",
    "    while open_set:\n",
    "        # Pop the node with the lowest f_cost from the open set\n",
    "        current_f_cost, current_g_cost, current_index = heappop(open_set)\n",
    "        # Add the current node to the explored set\n",
    "        explored.add(current_index)\n",
    "\n",
    "        # For directly reconstruct path\n",
    "        if current_index == goal_index:\n",
    "            break\n",
    "\n",
    "        # Get the neighbors of the current node \n",
    "        index_vector = ex_path[current_index,:]\n",
    "        neighbors_index = np.nonzero(index_vector)[0]\n",
    "\n",
    "        for index in neighbors_index:\n",
    "             if  index not in explored:\n",
    "                    # Calculate tentative_g_cost\n",
    "                    tentative_g_cost = current_g_cost + distance_matrix[current_index,index]\n",
    "\n",
    "                    # If this path to neighbor is better than any previous one\n",
    "                    if index not in g_costs or tentative_g_cost < g_costs[index]:\n",
    "                        # Update came_from, g_costs, and f_cost\n",
    "                        came_from[index] = current_index\n",
    "                        g_costs[index] = tentative_g_cost\n",
    "                        f_cost = tentative_g_cost + heuristic(points[index,:], goal)\n",
    "                        \n",
    "                        # Add neighbor to open set\n",
    "                        heappush(open_set, (f_cost, tentative_g_cost, index))\n",
    "                        operation_count += 1\n",
    "\n",
    "    # Reconstruct path\n",
    "    if current_index == goal_index:\n",
    "        path = []\n",
    "        while current_index in came_from:\n",
    "            path.append(points[current_index,:])\n",
    "            current_index = came_from[current_index]\n",
    "        path.append(start)\n",
    "        np_path = np.array(path)\n",
    "        return np_path[::-1]\n",
    "    else:\n",
    "        # If we reach here, no path was found\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photo qui montre le chemin optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Robot's actuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_direction(coordinates, nodes_slopes, segment_index):\n",
    "    \"\"\"\n",
    "    Find the right motor speed so the robot follows the rigth seglent\n",
    "\n",
    "    input: \n",
    "        coordinates: 1 x 3 array, where:\n",
    "            - The first row represents the mean of x, y and theta coordinates.\n",
    "        nodes_slopes = M x 5 (M = number of nodes in the shortest path), where \n",
    "        each row represents the node's coordinates along with the \n",
    "        alpha and beta coefficients, the last colomn represents the direction \n",
    "        of the slope.\n",
    "\n",
    "    output: \n",
    "        speed: 1 X 2 , contains the speed value for the left and right motor respectively. \n",
    "        segment_index: scalar indicationg the index of the segment the robot is on.\n",
    "        end: scalar, if equal to 1 we're at the final destination\n",
    "    \"\"\"\n",
    "\n",
    "    M = nodes_slopes.shape[0]\n",
    "    speed = np.zeros(2)\n",
    "    y_mean = coordinates[1]\n",
    "    x_mean = coordinates[0]\n",
    "    theta_mean = coordinates[2]\n",
    "\n",
    "    tolerance_norm = 25\n",
    "    angle_tolerance = 0.17 # in radian is almost equal 10°\n",
    "    Param1 = 0.5 \n",
    "    Param2 = 200 \n",
    "    Param3 = 150\n",
    "    end = 0\n",
    "\n",
    "    #check if we're already at the final distination\n",
    "    if segment_index == (M-1):\n",
    "        end = 1\n",
    "        speed[:] = [0,0]\n",
    "        return speed, segment_index, end    \n",
    "\n",
    "\n",
    "    #check if we're close to the end of the segment\n",
    "    distance_segm = ((nodes_slopes[segment_index+1,1]-y_mean)**2 + (nodes_slopes[segment_index+1,0]-x_mean)**2)**0.5\n",
    "\n",
    "    if distance_segm < tolerance_norm:\n",
    "        #means we're at the end of the segment\n",
    "        segment_index += 1\n",
    "\n",
    "    #check if we've reached the final distination\n",
    "    if segment_index == (M-1):\n",
    "        end = 1\n",
    "        speed[:] = [0,0]\n",
    "        return speed, segment_index, end    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #find the angle of the slope and set the speed\n",
    "    angle_err = angle_error(x_mean,y_mean, theta_mean, nodes_slopes[segment_index+1,0], nodes_slopes[segment_index+1,1])\n",
    "\n",
    "    if abs(angle_err) > angle_tolerance:\n",
    "        speed[0] = angle_err*Param2\n",
    "        speed[1] = -1*angle_err*Param2\n",
    "\n",
    "    else:    \n",
    "        speed[0] = distance_segm*Param1 + angle_err*Param2 + Param3\n",
    "        speed[1] = distance_segm*Param1 - angle_err*Param2 + Param3\n",
    "\n",
    "    return speed, segment_index, end\n",
    "\n",
    "\n",
    "# %%\n",
    "def angle_error(x_rob,y_rob, theta_rob, x_fin, y_fin):\n",
    "    \"\"\"\n",
    "    Find on which slope the robot is so we can activate the motors \n",
    "\n",
    "    input: \n",
    "        x_rob : x coordinates of the robot\n",
    "        y_rob : y coordinates of the robot\n",
    "        theta_rob : theta coordinates of the robot\n",
    "        x_fin : x coordinates of the end of the segment\n",
    "        y_fin : y coordinates of the end of the segment\n",
    "\n",
    "    output: \n",
    "        angle_err: scalar in radian corresponding to the difference between the orientation\n",
    "                   of the robot and the slope on which it should be.\n",
    "    \"\"\"\n",
    "\n",
    "    angle_slope = np.arctan2((y_fin-y_rob), (x_fin-x_rob))\n",
    "    diff_angle = angle_slope - theta_rob\n",
    "    angle_err = (diff_angle + np.pi) % (2 * np.pi) - np.pi\n",
    "    return angle_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local navigation concerns all the robot's reactions to the stimulation of its on-board sensors. Here, local navigation is triggered by the sudden appearance of obstacles on the map. The on-board sensors detect the obstacle, triggering avoidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Obstacle avoidance (Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of our local navigation is to avoid incoming obstacles that are not mapped by the computer vision. For that, we decided to use the Artificial Neural Network method presented in the \"Exercice Session 3\" of the EPFL's Mobile Robotics course. We chose this way of proceeding because it allows to consider the informations of all the proximity sensos all at once. Furthermore, it is a good way to adapt the wheels' speed depending on the closeness of an obstacle as the actuation grows linearly with the values of the sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Completion of the obstacle avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Position Estimation: Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Empirical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Kalman filtering theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Video Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Global example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Specific cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Obstacle avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Kidnapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Change in goal's position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. Hidden Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
